{
  "model": "kenshi777/qwen-1.7b-coder-4bit",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful coding assistant."
    },
    {
      "role": "user",
      "content": "Create a Dockerfile for a Python application that uses vLLM to serve a Qwen-1.7b model with 4-bit quantization."
    }
  ],
  "temperature": 0.7,
  "max_tokens": 256,
  "stream": false
}
